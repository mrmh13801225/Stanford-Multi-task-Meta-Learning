{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "import omniglot\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 1\n",
    "NUM_HIDDEN_CHANNELS = 64\n",
    "KERNEL_SIZE = 3\n",
    "NUM_CONV_LAYERS = 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SUMMARY_INTERVAL = 10\n",
    "SAVE_INTERVAL = 100\n",
    "PRINT_INTERVAL = 10\n",
    "VAL_INTERVAL = PRINT_INTERVAL * 5\n",
    "NUM_TEST_TASKS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNetNetwork(nn.Module):\n",
    "    \"\"\"Container for ProtoNet weights and image-to-latent computation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Inits ProtoNetNetwork.\n",
    "\n",
    "        The network consists of four convolutional blocks, each comprising a\n",
    "        convolution layer, a batch normalization layer, ReLU activation, and 2x2\n",
    "        max pooling for downsampling. There is an additional flattening\n",
    "        operation at the end.\n",
    "\n",
    "        Note that unlike conventional use, batch normalization is always done\n",
    "        with batch statistics, regardless of whether we are training or\n",
    "        evaluating. This technically makes meta-learning transductive, as\n",
    "        opposed to inductive.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels = NUM_INPUT_CHANNELS\n",
    "        for _ in range(NUM_CONV_LAYERS):\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    (KERNEL_SIZE, KERNEL_SIZE),\n",
    "                    padding='same'\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.BatchNorm2d(NUM_HIDDEN_CHANNELS))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "            in_channels = NUM_HIDDEN_CHANNELS\n",
    "        layers.append(nn.Flatten())\n",
    "        self._layers = nn.Sequential(*layers)\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Computes the latent representation of a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): batch of Omniglot images\n",
    "                shape (num_images, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            a Tensor containing a batch of latent representations\n",
    "                shape (num_images, latents)\n",
    "        \"\"\"\n",
    "        return self._layers(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet:\n",
    "    \"\"\"Trains and assesses a prototypical network.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate, log_dir):\n",
    "        \"\"\"Inits ProtoNet.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): learning rate for the Adam optimizer\n",
    "            log_dir (str): path to logging directory\n",
    "        \"\"\"\n",
    "\n",
    "        self._network = ProtoNetNetwork()\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            self._network.parameters(),\n",
    "            lr=learning_rate\n",
    "        )\n",
    "        self._log_dir = log_dir\n",
    "        os.makedirs(self._log_dir, exist_ok=True)\n",
    "\n",
    "        self._start_train_step = 0\n",
    "\n",
    "    def _step(self, task_batch):\n",
    "        \"\"\"Computes ProtoNet mean loss (and accuracy) on a batch of tasks.\n",
    "\n",
    "        Args:\n",
    "            task_batch (tuple[Tensor, Tensor, Tensor, Tensor]):\n",
    "                batch of tasks from an Omniglot DataLoader\n",
    "\n",
    "        Returns:\n",
    "            a Tensor containing mean ProtoNet loss over the batch\n",
    "                shape ()\n",
    "            mean support set accuracy over the batch as a float\n",
    "            mean query set accuracy over the batch as a float\n",
    "        \"\"\"\n",
    "        loss_batch = []\n",
    "        accuracy_support_batch = []\n",
    "        accuracy_query_batch = []\n",
    "        for task in task_batch:\n",
    "            images_support, labels_support, images_query, labels_query = task\n",
    "            images_support = images_support.to(DEVICE)\n",
    "            labels_support = labels_support.to(DEVICE)\n",
    "            images_query = images_query.to(DEVICE)\n",
    "            labels_query = labels_query.to(DEVICE)\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "            # TODO: finish implementing this method.\n",
    "            # For a given task, compute the prototypes and the protonet loss.\n",
    "            # Use F.cross_entropy to compute classification losses.\n",
    "            # Use util.score to compute accuracies.\n",
    "            # Make sure to populate loss_batch, accuracy_support_batch, and\n",
    "            # accuracy_query_batch.\n",
    "\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "        return (\n",
    "            torch.mean(torch.stack(loss_batch)),\n",
    "            np.mean(accuracy_support_batch),\n",
    "            np.mean(accuracy_query_batch)\n",
    "        )\n",
    "\n",
    "    def train(self, dataloader_train, dataloader_val, writer):\n",
    "        \"\"\"Train the ProtoNet.\n",
    "\n",
    "        Consumes dataloader_train to optimize weights of ProtoNetNetwork\n",
    "        while periodically validating on dataloader_val, logging metrics, and\n",
    "        saving checkpoints.\n",
    "\n",
    "        Args:\n",
    "            dataloader_train (DataLoader): loader for train tasks\n",
    "            dataloader_val (DataLoader): loader for validation tasks\n",
    "            writer (SummaryWriter): TensorBoard logger\n",
    "        \"\"\"\n",
    "        print(f'Starting training at iteration {self._start_train_step}.')\n",
    "        for i_step, task_batch in enumerate(\n",
    "                dataloader_train,\n",
    "                start=self._start_train_step\n",
    "        ):\n",
    "            self._optimizer.zero_grad()\n",
    "            loss, accuracy_support, accuracy_query = self._step(task_batch)\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            if i_step % PRINT_INTERVAL == 0:\n",
    "                print(\n",
    "                    f'Iteration {i_step}: '\n",
    "                    f'loss: {loss.item():.3f}, '\n",
    "                    f'support accuracy: {accuracy_support.item():.3f}, '\n",
    "                    f'query accuracy: {accuracy_query.item():.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/train', loss.item(), i_step)\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/support',\n",
    "                    accuracy_support.item(),\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/query',\n",
    "                    accuracy_query.item(),\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % VAL_INTERVAL == 0:\n",
    "                with torch.no_grad():\n",
    "                    losses, accuracies_support, accuracies_query = [], [], []\n",
    "                    for val_task_batch in dataloader_val:\n",
    "                        loss, accuracy_support, accuracy_query = (\n",
    "                            self._step(val_task_batch)\n",
    "                        )\n",
    "                        losses.append(loss.item())\n",
    "                        accuracies_support.append(accuracy_support)\n",
    "                        accuracies_query.append(accuracy_query)\n",
    "                    loss = np.mean(losses)\n",
    "                    accuracy_support = np.mean(accuracies_support)\n",
    "                    accuracy_query = np.mean(accuracies_query)\n",
    "                print(\n",
    "                    f'Validation: '\n",
    "                    f'loss: {loss:.3f}, '\n",
    "                    f'support accuracy: {accuracy_support:.3f}, '\n",
    "                    f'query accuracy: {accuracy_query:.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/val', loss, i_step)\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/support',\n",
    "                    accuracy_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/query',\n",
    "                    accuracy_query,\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % SAVE_INTERVAL == 0:\n",
    "                self._save(i_step)\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        \"\"\"Evaluate the ProtoNet on test tasks.\n",
    "\n",
    "        Args:\n",
    "            dataloader_test (DataLoader): loader for test tasks\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        for task_batch in dataloader_test:\n",
    "            accuracies.append(self._step(task_batch)[2])\n",
    "        mean = np.mean(accuracies)\n",
    "        std = np.std(accuracies)\n",
    "        mean_95_confidence_interval = 1.96 * std / np.sqrt(NUM_TEST_TASKS)\n",
    "        print(\n",
    "            f'Accuracy over {NUM_TEST_TASKS} test tasks: '\n",
    "            f'mean {mean:.3f}, '\n",
    "            f'95% confidence interval {mean_95_confidence_interval:.3f}'\n",
    "        )\n",
    "\n",
    "    def load(self, checkpoint_step):\n",
    "        \"\"\"Loads a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration of checkpoint to load\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if checkpoint for checkpoint_step is not found\n",
    "        \"\"\"\n",
    "        target_path = (\n",
    "            f'{os.path.join(self._log_dir, \"state\")}'\n",
    "            f'{checkpoint_step}.pt'\n",
    "        )\n",
    "        if os.path.isfile(target_path):\n",
    "            state = torch.load(target_path)\n",
    "            self._network.load_state_dict(state['network_state_dict'])\n",
    "            self._optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            self._start_train_step = checkpoint_step + 1\n",
    "            print(f'Loaded checkpoint iteration {checkpoint_step}.')\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'No checkpoint for iteration {checkpoint_step} found.'\n",
    "            )\n",
    "\n",
    "    def _save(self, checkpoint_step):\n",
    "        \"\"\"Saves network and optimizer state_dicts as a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration to label checkpoint with\n",
    "        \"\"\"\n",
    "        torch.save(\n",
    "            dict(network_state_dict=self._network.state_dict(),\n",
    "                 optimizer_state_dict=self._optimizer.state_dict()),\n",
    "            f'{os.path.join(self._log_dir, \"state\")}{checkpoint_step}.pt'\n",
    "        )\n",
    "        print('Saved checkpoint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    log_dir = args.log_dir\n",
    "    if log_dir is None:\n",
    "        log_dir = f'./logs/protonet/omniglot.way:{args.num_way}.support:{args.num_support}.query:{args.num_query}.lr:{args.learning_rate}.batch_size:{args.batch_size}'  # pylint: disable=line-too-long\n",
    "    print(f'log_dir: {log_dir}')\n",
    "    writer = tensorboard.SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    protonet = ProtoNet(args.learning_rate, log_dir)\n",
    "\n",
    "    if args.checkpoint_step > -1:\n",
    "        protonet.load(args.checkpoint_step)\n",
    "    else:\n",
    "        print('Checkpoint loading skipped.')\n",
    "\n",
    "    if not args.test:\n",
    "        num_training_tasks = args.batch_size * (args.num_train_iterations -\n",
    "                                                args.checkpoint_step - 1)\n",
    "        print(\n",
    "            f'Training on tasks with composition '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_train = omniglot.get_omniglot_dataloader(\n",
    "            'train',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            num_training_tasks\n",
    "        )\n",
    "        dataloader_val = omniglot.get_omniglot_dataloader(\n",
    "            'val',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            args.batch_size * 4\n",
    "        )\n",
    "        protonet.train(\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "            writer\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f'Testing on tasks with composition '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_test = omniglot.get_omniglot_dataloader(\n",
    "            'test',\n",
    "            1,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            NUM_TEST_TASKS\n",
    "        )\n",
    "        protonet.test(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Train a ProtoNet! [-h] [--log_dir LOG_DIR] [--num_way NUM_WAY]\n",
      "                         [--num_support NUM_SUPPORT] [--num_query NUM_QUERY]\n",
      "                         [--learning_rate LEARNING_RATE]\n",
      "                         [--batch_size BATCH_SIZE]\n",
      "                         [--num_train_iterations NUM_TRAIN_ITERATIONS]\n",
      "                         [--test] [--checkpoint_step CHECKPOINT_STEP]\n",
      "Train a ProtoNet!: error: unrecognized arguments: --f=c:\\Users\\mmd\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-7320fox0I29Xgh1w.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaco\\envs\\GPU2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('Train a ProtoNet!')\n",
    "    parser.add_argument('--log_dir', type=str, default=None,\n",
    "                        help='directory to save to or load from')\n",
    "    parser.add_argument('--num_way', type=int, default=5,\n",
    "                        help='number of classes in a task')\n",
    "    parser.add_argument('--num_support', type=int, default=1,\n",
    "                        help='number of support examples per class in a task')\n",
    "    parser.add_argument('--num_query', type=int, default=15,\n",
    "                        help='number of query examples per class in a task')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                        help='learning rate for the network')\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "                        help='number of tasks per outer-loop update')\n",
    "    parser.add_argument('--num_train_iterations', type=int, default=5000,\n",
    "                        help='number of outer-loop updates to train for')\n",
    "    parser.add_argument('--test', default=False, action='store_true',\n",
    "                        help='train or test')\n",
    "    parser.add_argument('--checkpoint_step', type=int, default=-1,\n",
    "                        help=('checkpoint iteration to load for resuming '\n",
    "                              'training, or for evaluation (-1 is ignored)'))\n",
    "\n",
    "    main_args = parser.parse_args()\n",
    "    main(main_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
