{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "import omniglot\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUT_CHANNELS = 1\n",
    "NUM_HIDDEN_CHANNELS = 64\n",
    "KERNEL_SIZE = 3\n",
    "NUM_CONV_LAYERS = 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SUMMARY_INTERVAL = 10\n",
    "SAVE_INTERVAL = 100\n",
    "LOG_INTERVAL = 10\n",
    "VAL_INTERVAL = LOG_INTERVAL * 5\n",
    "NUM_TEST_TASKS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML:\n",
    "    \"\"\"Trains and assesses a MAML.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_outputs,\n",
    "            num_inner_steps,\n",
    "            inner_lr,\n",
    "            learn_inner_lrs,\n",
    "            outer_lr,\n",
    "            log_dir\n",
    "    ):\n",
    "        \"\"\"Inits MAML.\n",
    "\n",
    "        The network consists of four convolutional blocks followed by a linear\n",
    "        head layer. Each convolutional block comprises a convolution layer, a\n",
    "        batch normalization layer, and ReLU activation.\n",
    "\n",
    "        Note that unlike conventional use, batch normalization is always done\n",
    "        with batch statistics, regardless of whether we are training or\n",
    "        evaluating. This technically makes meta-learning transductive, as\n",
    "        opposed to inductive.\n",
    "\n",
    "        Args:\n",
    "            num_outputs (int): dimensionality of output, i.e. number of classes\n",
    "                in a task\n",
    "            num_inner_steps (int): number of inner-loop optimization steps\n",
    "            inner_lr (float): learning rate for inner-loop optimization\n",
    "                If learn_inner_lrs=True, inner_lr serves as the initialization\n",
    "                of the learning rates.\n",
    "            learn_inner_lrs (bool): whether to learn the above\n",
    "            outer_lr (float): learning rate for outer-loop optimization\n",
    "            log_dir (str): path to logging directory\n",
    "        \"\"\"\n",
    "        meta_parameters = {}\n",
    "\n",
    "        # construct feature extractor\n",
    "        in_channels = NUM_INPUT_CHANNELS\n",
    "        for i in range(NUM_CONV_LAYERS):\n",
    "            meta_parameters[f'conv{i}'] = nn.init.xavier_uniform_(\n",
    "                torch.empty(\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    in_channels,\n",
    "                    KERNEL_SIZE,\n",
    "                    KERNEL_SIZE,\n",
    "                    requires_grad=True,\n",
    "                    device=DEVICE\n",
    "                )\n",
    "            )\n",
    "            meta_parameters[f'b{i}'] = nn.init.zeros_(\n",
    "                torch.empty(\n",
    "                    NUM_HIDDEN_CHANNELS,\n",
    "                    requires_grad=True,\n",
    "                    device=DEVICE\n",
    "                )\n",
    "            )\n",
    "            in_channels = NUM_HIDDEN_CHANNELS\n",
    "\n",
    "        # construct linear head layer\n",
    "        meta_parameters[f'w{NUM_CONV_LAYERS}'] = nn.init.xavier_uniform_(\n",
    "            torch.empty(\n",
    "                num_outputs,\n",
    "                NUM_HIDDEN_CHANNELS,\n",
    "                requires_grad=True,\n",
    "                device=DEVICE\n",
    "            )\n",
    "        )\n",
    "        meta_parameters[f'b{NUM_CONV_LAYERS}'] = nn.init.zeros_(\n",
    "            torch.empty(\n",
    "                num_outputs,\n",
    "                requires_grad=True,\n",
    "                device=DEVICE\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._meta_parameters = meta_parameters\n",
    "        self._num_inner_steps = num_inner_steps\n",
    "        self._inner_lrs = {\n",
    "            k: torch.tensor(inner_lr, requires_grad=learn_inner_lrs)\n",
    "            for k in self._meta_parameters.keys()\n",
    "        }\n",
    "        self._outer_lr = outer_lr\n",
    "\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            list(self._meta_parameters.values()) +\n",
    "            list(self._inner_lrs.values()),\n",
    "            lr=self._outer_lr\n",
    "        )\n",
    "        self._log_dir = log_dir\n",
    "        os.makedirs(self._log_dir, exist_ok=True)\n",
    "\n",
    "        self._start_train_step = 0\n",
    "\n",
    "    def _forward(self, images, parameters):\n",
    "        \"\"\"Computes predicted classification logits.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): batch of Omniglot images\n",
    "                shape (num_images, channels, height, width)\n",
    "            parameters (dict[str, Tensor]): parameters to use for\n",
    "                the computation\n",
    "\n",
    "        Returns:\n",
    "            a Tensor consisting of a batch of logits\n",
    "                shape (num_images, classes)\n",
    "        \"\"\"\n",
    "        x = images\n",
    "        for i in range(NUM_CONV_LAYERS):\n",
    "            x = F.conv2d(\n",
    "                input=x,\n",
    "                weight=parameters[f'conv{i}'],\n",
    "                bias=parameters[f'b{i}'],\n",
    "                stride=1,\n",
    "                padding='same'\n",
    "            )\n",
    "            x = F.batch_norm(x, None, None, training=True)\n",
    "            x = F.relu(x)\n",
    "        x = torch.mean(x, dim=[2, 3])\n",
    "        return F.linear(\n",
    "            input=x,\n",
    "            weight=parameters[f'w{NUM_CONV_LAYERS}'],\n",
    "            bias=parameters[f'b{NUM_CONV_LAYERS}']\n",
    "        )\n",
    "\n",
    "    def _inner_loop(self, images, labels, train):\n",
    "        \"\"\"Computes the adapted network parameters via the MAML inner loop.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): task support set inputs\n",
    "                shape (num_images, channels, height, width)\n",
    "            labels (Tensor): task support set outputs\n",
    "                shape (num_images,)\n",
    "            train (bool): whether we are training or evaluating\n",
    "\n",
    "        Returns:\n",
    "            parameters (dict[str, Tensor]): adapted network parameters\n",
    "            accuracies (list[float]): support set accuracy over the course of\n",
    "                the inner loop, length num_inner_steps + 1\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        parameters = {\n",
    "            k: torch.clone(v)\n",
    "            for k, v in self._meta_parameters.items()\n",
    "        }\n",
    "        # ********************************************************\n",
    "        # ******************* YOUR CODE HERE *********************\n",
    "        # ********************************************************\n",
    "        # TODO: finish implementing this method.\n",
    "        # This method computes the inner loop (adaptation) procedure\n",
    "        # over the course of _num_inner_steps steps for one\n",
    "        # task. It also scores the model along the way.\n",
    "        # Make sure to populate accuracies and update parameters.\n",
    "        # Use F.cross_entropy to compute classification losses.\n",
    "        # Use util.score to compute accuracies.\n",
    "\n",
    "        # ********************************************************\n",
    "        # ******************* YOUR CODE HERE *********************\n",
    "        # ********************************************************\n",
    "        return parameters, accuracies\n",
    "\n",
    "    def _outer_step(self, task_batch, train):\n",
    "        \"\"\"Computes the MAML loss and metrics on a batch of tasks.\n",
    "\n",
    "        Args:\n",
    "            task_batch (tuple): batch of tasks from an Omniglot DataLoader\n",
    "            train (bool): whether we are training or evaluating\n",
    "\n",
    "        Returns:\n",
    "            outer_loss (Tensor): mean MAML loss over the batch, scalar\n",
    "            accuracies_support (ndarray): support set accuracy over the\n",
    "                course of the inner loop, averaged over the task batch\n",
    "                shape (num_inner_steps + 1,)\n",
    "            accuracy_query (float): query set accuracy of the adapted\n",
    "                parameters, averaged over the task batch\n",
    "        \"\"\"\n",
    "        outer_loss_batch = []\n",
    "        accuracies_support_batch = []\n",
    "        accuracy_query_batch = []\n",
    "        for task in task_batch:\n",
    "            images_support, labels_support, images_query, labels_query = task\n",
    "            images_support = images_support.to(DEVICE)\n",
    "            labels_support = labels_support.to(DEVICE)\n",
    "            images_query = images_query.to(DEVICE)\n",
    "            labels_query = labels_query.to(DEVICE)\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "            # TODO: finish implementing this method.\n",
    "            # For a given task, use the _inner_loop method to adapt for\n",
    "            # _num_inner_steps steps, then compute the MAML loss and other\n",
    "            # metrics.\n",
    "            # Use F.cross_entropy to compute classification losses.\n",
    "            # Use util.score to compute accuracies.\n",
    "            # Make sure to populate outer_loss_batch, accuracies_support_batch,\n",
    "            # and accuracy_query_batch.\n",
    "\n",
    "            # ********************************************************\n",
    "            # ******************* YOUR CODE HERE *********************\n",
    "            # ********************************************************\n",
    "        outer_loss = torch.mean(torch.stack(outer_loss_batch))\n",
    "        accuracies_support = np.mean(\n",
    "            accuracies_support_batch,\n",
    "            axis=0\n",
    "        )\n",
    "        accuracy_query = np.mean(accuracy_query_batch)\n",
    "        return outer_loss, accuracies_support, accuracy_query\n",
    "\n",
    "    def train(self, dataloader_train, dataloader_val, writer):\n",
    "        \"\"\"Train the MAML.\n",
    "\n",
    "        Consumes dataloader_train to optimize MAML meta-parameters\n",
    "        while periodically validating on dataloader_val, logging metrics, and\n",
    "        saving checkpoints.\n",
    "\n",
    "        Args:\n",
    "            dataloader_train (DataLoader): loader for train tasks\n",
    "            dataloader_val (DataLoader): loader for validation tasks\n",
    "            writer (SummaryWriter): TensorBoard logger\n",
    "        \"\"\"\n",
    "        print(f'Starting training at iteration {self._start_train_step}.')\n",
    "        for i_step, task_batch in enumerate(\n",
    "                dataloader_train,\n",
    "                start=self._start_train_step\n",
    "        ):\n",
    "            self._optimizer.zero_grad()\n",
    "            outer_loss, accuracies_support, accuracy_query = (\n",
    "                self._outer_step(task_batch, train=True)\n",
    "            )\n",
    "            outer_loss.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "            if i_step % LOG_INTERVAL == 0:\n",
    "                print(\n",
    "                    f'Iteration {i_step}: '\n",
    "                    f'loss: {outer_loss.item():.3f}, '\n",
    "                    f'pre-adaptation support accuracy: '\n",
    "                    f'{accuracies_support[0]:.3f}, '\n",
    "                    f'post-adaptation support accuracy: '\n",
    "                    f'{accuracies_support[-1]:.3f}, '\n",
    "                    f'post-adaptation query accuracy: '\n",
    "                    f'{accuracy_query:.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/train', outer_loss.item(), i_step)\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/pre_adapt_support',\n",
    "                    accuracies_support[0],\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/post_adapt_support',\n",
    "                    accuracies_support[-1],\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'train_accuracy/post_adapt_query',\n",
    "                    accuracy_query,\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % VAL_INTERVAL == 0:\n",
    "                losses = []\n",
    "                accuracies_pre_adapt_support = []\n",
    "                accuracies_post_adapt_support = []\n",
    "                accuracies_post_adapt_query = []\n",
    "                for val_task_batch in dataloader_val:\n",
    "                    outer_loss, accuracies_support, accuracy_query = (\n",
    "                        self._outer_step(val_task_batch, train=False)\n",
    "                    )\n",
    "                    losses.append(outer_loss.item())\n",
    "                    accuracies_pre_adapt_support.append(accuracies_support[0])\n",
    "                    accuracies_post_adapt_support.append(accuracies_support[-1])\n",
    "                    accuracies_post_adapt_query.append(accuracy_query)\n",
    "                loss = np.mean(losses)\n",
    "                accuracy_pre_adapt_support = np.mean(\n",
    "                    accuracies_pre_adapt_support\n",
    "                )\n",
    "                accuracy_post_adapt_support = np.mean(\n",
    "                    accuracies_post_adapt_support\n",
    "                )\n",
    "                accuracy_post_adapt_query = np.mean(\n",
    "                    accuracies_post_adapt_query\n",
    "                )\n",
    "                print(\n",
    "                    f'Validation: '\n",
    "                    f'loss: {loss:.3f}, '\n",
    "                    f'pre-adaptation support accuracy: '\n",
    "                    f'{accuracy_pre_adapt_support:.3f}, '\n",
    "                    f'post-adaptation support accuracy: '\n",
    "                    f'{accuracy_post_adapt_support:.3f}, '\n",
    "                    f'post-adaptation query accuracy: '\n",
    "                    f'{accuracy_post_adapt_query:.3f}'\n",
    "                )\n",
    "                writer.add_scalar('loss/val', loss, i_step)\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/pre_adapt_support',\n",
    "                    accuracy_pre_adapt_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/post_adapt_support',\n",
    "                    accuracy_post_adapt_support,\n",
    "                    i_step\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    'val_accuracy/post_adapt_query',\n",
    "                    accuracy_post_adapt_query,\n",
    "                    i_step\n",
    "                )\n",
    "\n",
    "            if i_step % SAVE_INTERVAL == 0:\n",
    "                self._save(i_step)\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        \"\"\"Evaluate the MAML on test tasks.\n",
    "\n",
    "        Args:\n",
    "            dataloader_test (DataLoader): loader for test tasks\n",
    "        \"\"\"\n",
    "        accuracies = []\n",
    "        for task_batch in dataloader_test:\n",
    "            _, _, accuracy_query = self._outer_step(task_batch, train=False)\n",
    "            accuracies.append(accuracy_query)\n",
    "        mean = np.mean(accuracies)\n",
    "        std = np.std(accuracies)\n",
    "        mean_95_confidence_interval = 1.96 * std / np.sqrt(NUM_TEST_TASKS)\n",
    "        print(\n",
    "            f'Accuracy over {NUM_TEST_TASKS} test tasks: '\n",
    "            f'mean {mean:.3f}, '\n",
    "            f'95% confidence interval {mean_95_confidence_interval:.3f}'\n",
    "        )\n",
    "\n",
    "    def load(self, checkpoint_step):\n",
    "        \"\"\"Loads a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration of checkpoint to load\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if checkpoint for checkpoint_step is not found\n",
    "        \"\"\"\n",
    "        target_path = (\n",
    "            f'{os.path.join(self._log_dir, \"state\")}'\n",
    "            f'{checkpoint_step}.pt'\n",
    "        )\n",
    "        if os.path.isfile(target_path):\n",
    "            state = torch.load(target_path)\n",
    "            self._meta_parameters = state['meta_parameters']\n",
    "            self._inner_lrs = state['inner_lrs']\n",
    "            self._optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            self._start_train_step = checkpoint_step + 1\n",
    "            print(f'Loaded checkpoint iteration {checkpoint_step}.')\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'No checkpoint for iteration {checkpoint_step} found.'\n",
    "            )\n",
    "\n",
    "    def _save(self, checkpoint_step):\n",
    "        \"\"\"Saves parameters and optimizer state_dict as a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_step (int): iteration to label checkpoint with\n",
    "        \"\"\"\n",
    "        optimizer_state_dict = self._optimizer.state_dict()\n",
    "        torch.save(\n",
    "            dict(meta_parameters=self._meta_parameters,\n",
    "                 inner_lrs=self._inner_lrs,\n",
    "                 optimizer_state_dict=optimizer_state_dict),\n",
    "            f'{os.path.join(self._log_dir, \"state\")}{checkpoint_step}.pt'\n",
    "        )\n",
    "        print('Saved checkpoint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    log_dir = args.log_dir\n",
    "    if log_dir is None:\n",
    "        log_dir = f'./logs/maml/omniglot.way:{args.num_way}.support:{args.num_support}.query:{args.num_query}.inner_steps:{args.num_inner_steps}.inner_lr:{args.inner_lr}.learn_inner_lrs:{args.learn_inner_lrs}.outer_lr:{args.outer_lr}.batch_size:{args.batch_size}'  # pylint: disable=line-too-long\n",
    "    print(f'log_dir: {log_dir}')\n",
    "    writer = tensorboard.SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    maml = MAML(\n",
    "        args.num_way,\n",
    "        args.num_inner_steps,\n",
    "        args.inner_lr,\n",
    "        args.learn_inner_lrs,\n",
    "        args.outer_lr,\n",
    "        log_dir\n",
    "    )\n",
    "\n",
    "    if args.checkpoint_step > -1:\n",
    "        maml.load(args.checkpoint_step)\n",
    "    else:\n",
    "        print('Checkpoint loading skipped.')\n",
    "\n",
    "    if not args.test:\n",
    "        num_training_tasks = args.batch_size * (args.num_train_iterations -\n",
    "                                                args.checkpoint_step - 1)\n",
    "        print(\n",
    "            f'Training on {num_training_tasks} tasks with composition: '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_train = omniglot.get_omniglot_dataloader(\n",
    "            'train',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            num_training_tasks\n",
    "        )\n",
    "        dataloader_val = omniglot.get_omniglot_dataloader(\n",
    "            'val',\n",
    "            args.batch_size,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            args.batch_size * 4\n",
    "        )\n",
    "        maml.train(\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "            writer\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f'Testing on tasks with composition '\n",
    "            f'num_way={args.num_way}, '\n",
    "            f'num_support={args.num_support}, '\n",
    "            f'num_query={args.num_query}'\n",
    "        )\n",
    "        dataloader_test = omniglot.get_omniglot_dataloader(\n",
    "            'test',\n",
    "            1,\n",
    "            args.num_way,\n",
    "            args.num_support,\n",
    "            args.num_query,\n",
    "            NUM_TEST_TASKS\n",
    "        )\n",
    "        maml.test(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Train a MAML! [-h] [--log_dir LOG_DIR] [--num_way NUM_WAY]\n",
      "                     [--num_support NUM_SUPPORT] [--num_query NUM_QUERY]\n",
      "                     [--num_inner_steps NUM_INNER_STEPS] [--inner_lr INNER_LR]\n",
      "                     [--learn_inner_lrs] [--outer_lr OUTER_LR]\n",
      "                     [--batch_size BATCH_SIZE]\n",
      "                     [--num_train_iterations NUM_TRAIN_ITERATIONS] [--test]\n",
      "                     [--checkpoint_step CHECKPOINT_STEP]\n",
      "Train a MAML!: error: unrecognized arguments: --f=c:\\Users\\mmd\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-73205wre2FrUavhn.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaco\\envs\\GPU2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('Train a MAML!')\n",
    "    parser.add_argument('--log_dir', type=str, default=None,\n",
    "                        help='directory to save to or load from')\n",
    "    parser.add_argument('--num_way', type=int, default=5,\n",
    "                        help='number of classes in a task')\n",
    "    parser.add_argument('--num_support', type=int, default=1,\n",
    "                        help='number of support examples per class in a task')\n",
    "    parser.add_argument('--num_query', type=int, default=15,\n",
    "                        help='number of query examples per class in a task')\n",
    "    parser.add_argument('--num_inner_steps', type=int, default=1,\n",
    "                        help='number of inner-loop updates')\n",
    "    parser.add_argument('--inner_lr', type=float, default=0.4,\n",
    "                        help='inner-loop learning rate initialization')\n",
    "    parser.add_argument('--learn_inner_lrs', default=False, action='store_true',\n",
    "                        help='whether to optimize inner-loop learning rates')\n",
    "    parser.add_argument('--outer_lr', type=float, default=0.001,\n",
    "                        help='outer-loop learning rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "                        help='number of tasks per outer-loop update')\n",
    "    parser.add_argument('--num_train_iterations', type=int, default=15000,\n",
    "                        help='number of outer-loop updates to train for')\n",
    "    parser.add_argument('--test', default=False, action='store_true',\n",
    "                        help='train or test')\n",
    "    parser.add_argument('--checkpoint_step', type=int, default=-1,\n",
    "                        help=('checkpoint iteration to load for resuming '\n",
    "                              'training, or for evaluation (-1 is ignored)'))\n",
    "\n",
    "    main_args = parser.parse_args()\n",
    "    main(main_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
