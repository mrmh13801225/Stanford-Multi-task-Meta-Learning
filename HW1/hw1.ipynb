{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from load_data import DataGenerator\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    if type(model) in [nn.Linear]:\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        nn.init.zeros_(model.bias)\n",
    "    elif type(model) in [nn.LSTM, nn.RNN, nn.GRU]:\n",
    "        nn.init.orthogonal_(model.weight_hh_l0)\n",
    "        nn.init.xavier_uniform_(model.weight_ih_l0)\n",
    "        nn.init.zeros_(model.bias_hh_l0)\n",
    "        nn.init.zeros_(model.bias_ih_l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MANN(nn.Module):\n",
    "    def __init__(self, num_classes, samples_per_class, hidden_dim):\n",
    "        super(MANN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.samples_per_class = samples_per_class\n",
    "\n",
    "        self.layer1 = torch.nn.LSTM(num_classes + 784, hidden_dim, batch_first=True)\n",
    "        self.layer2 = torch.nn.LSTM(hidden_dim, num_classes, batch_first=True)\n",
    "        initialize_weights(self.layer1)\n",
    "        initialize_weights(self.layer2)\n",
    "\n",
    "    def forward(self, input_images, input_labels):\n",
    "        \"\"\"\n",
    "        MANN\n",
    "        Args:\n",
    "            input_images: [B, K+1, N, 784] flattened images\n",
    "            labels: [B, K+1, N, N] ground truth labels\n",
    "        Returns:\n",
    "            [B, K+1, N, N] predictions\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #############################\n",
    "        \n",
    "        B, K_plus_1, N, _ = input_images.size()\n",
    "    \n",
    "        predictions = torch.zeros(B, K_plus_1, N, N, device=input_images.device)\n",
    "        \n",
    "        for b in range(B):\n",
    "            for k in range(K_plus_1):\n",
    "                for n in range(N):\n",
    "                    nn_input = torch.cat((input_images[b, k, n].unsqueeze(0), input_labels[b, k, n].unsqueeze(0)), dim=1)\n",
    "                    hidden_states1, _ = self.layer1(nn_input)\n",
    "                    prediction, _ = self.layer2(hidden_states1)\n",
    "                    predictions[b, k, n] = prediction.squeeze(0).squeeze(0)\n",
    "    \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def loss_function(self, preds, labels):\n",
    "        \"\"\"\n",
    "        Computes MANN loss\n",
    "        Args:\n",
    "            preds: [B, K+1, N, N] network output\n",
    "            labels: [B, K+1, N, N] labels\n",
    "        Returns:\n",
    "            scalar loss\n",
    "        Note:\n",
    "            Loss should only be calculated on the N test images\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #############################\n",
    "        B, K_plus_1, N, _ = preds.shape\n",
    "        preds_test = preds[:, -1, :, :]\n",
    "        labels_test = labels[:, -1, :, :]\n",
    "\n",
    "        preds_flat = preds_test.reshape(B * N, N)\n",
    "        labels_flat = labels_test.reshape(B * N, N)\n",
    "\n",
    "        labels_flat = torch.argmax(labels_flat, dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(preds_flat, labels_flat)\n",
    "\n",
    "        return loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels, model, optim, eval=False):\n",
    "    predictions = model(images, labels)\n",
    "    loss = model.loss_function(predictions, labels)\n",
    "    if not eval:\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return predictions.detach(), loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    print(config)\n",
    "    random.seed(config.random_seed)\n",
    "    np.random.seed(config.random_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    writer = SummaryWriter(\n",
    "        f\"runs/{config.num_classes}_{config.num_shot}_{config.random_seed}_{config.hidden_dim}\"\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(\"./omniglot_resized\"):\n",
    "        gdd.download_file_from_google_drive(\n",
    "            file_id=\"1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI\",\n",
    "            dest_path=\"./omniglot_resized.zip\",\n",
    "            unzip=True,\n",
    "        )\n",
    "    assert os.path.isdir(\"./omniglot_resized\")\n",
    "\n",
    "    train_iterable = DataGenerator(\n",
    "        config.num_classes,\n",
    "        config.num_shot + 1,\n",
    "        batch_type=\"train\",\n",
    "        device=device,\n",
    "        cache=config.image_caching,\n",
    "    )\n",
    "    train_loader = iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            train_iterable,\n",
    "            batch_size=config.meta_batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    test_iterable = DataGenerator(\n",
    "        config.num_classes,\n",
    "        config.num_shot + 1,\n",
    "        batch_type=\"test\",\n",
    "        device=device,\n",
    "        cache=config.image_caching,\n",
    "    )\n",
    "    test_loader = iter(\n",
    "        torch.utils.data.DataLoader(\n",
    "            test_iterable,\n",
    "            batch_size=config.meta_batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = MANN(config.num_classes, config.num_shot + 1, config.hidden_dim)\n",
    "    model.to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    times = []\n",
    "    for step in tqdm(range(config.train_steps), desc=\"Training\"):\n",
    "        t0 = time.time()\n",
    "        i, l = next(train_loader)\n",
    "        i, l = i.to(device), l.to(device)\n",
    "        t1 = time.time()\n",
    "\n",
    "        _, ls = train_step(i, l, model, optim)\n",
    "        t2 = time.time()\n",
    "        writer.add_scalar(\"Loss/train\", ls, step)\n",
    "        times.append([t1 - t0, t2 - t1])\n",
    "\n",
    "        if (step + 1) % config.eval_freq == 0:\n",
    "            print(\"*\" * 5 + \"Iter \" + str(step + 1) + \"*\" * 5)\n",
    "            i, l = next(test_loader)\n",
    "            i, l = i.to(device), l.to(device)\n",
    "            pred, tls = train_step(i, l, model, optim, eval=True)\n",
    "            print(\"Train Loss:\", ls.cpu().numpy(), \"Test Loss:\", tls.cpu().numpy())\n",
    "            writer.add_scalar(\"Loss/test\", tls, step)\n",
    "            pred = torch.reshape(\n",
    "                pred, [-1, config.num_shot + 1, config.num_classes, config.num_classes]\n",
    "            )\n",
    "            pred = torch.argmax(pred[:, -1, :, :], axis=2)\n",
    "            l = torch.argmax(l[:, -1, :, :], axis=2)\n",
    "            acc = pred.eq(l).sum().item() / (config.meta_batch_size * config.num_classes)\n",
    "            print(\"Test Accuracy\", acc)\n",
    "            writer.add_scalar(\"Accuracy/test\", acc, step)\n",
    "\n",
    "            times = np.array(times)\n",
    "            print(f\"Sample time {times[:, 0].mean()} Train time {times[:, 1].mean()}\")\n",
    "            times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Config object at 0x0000025BFB49B6A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 12/25000 [01:03<36:27:41,  5.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     image_caching \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     13\u001b[0m config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 66\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     63\u001b[0m i, l \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mto(device), l\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     64\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 66\u001b[0m _, ls \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     68\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, ls, step)\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(images, labels, model, optim, eval)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28meval\u001b[39m:\n\u001b[0;32m      5\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mdetach(), loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\mmd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    num_classes = 5\n",
    "    num_shot = 1\n",
    "    num_workers = 4\n",
    "    eval_freq = 100\n",
    "    meta_batch_size = 128\n",
    "    hidden_dim = 128\n",
    "    random_seed = 123\n",
    "    learning_rate = 1e-3\n",
    "    train_steps = 25000\n",
    "    image_caching = True\n",
    "\n",
    "config = Config()\n",
    "main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 10 11 12]\n",
      " [21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "mmd = [\n",
    "    [[1,2,3,4],[5,6,7,8],[9,10,11,12]],\n",
    "    [[13,14,15,16],[17,18,19,20],[21,22,23,24]]\n",
    "    ] # b = 2 , k = 3 n = 4\n",
    "mmd_np = np.array(mmd)\n",
    "print (mmd_np[:,-1,:]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
